{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) chargement de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#sans se connecter au cluster\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"2g\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) lire de la donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) lecture brute\n",
    "Chargez le fichier ville_1.csv dans une variable nommée df.\n",
    "\n",
    "Vous pouvez afficher votre donnée en utilisant la méthode take() ou la methode collect() de l'objet pyspark DataFrame (attention appeler collect() sur un dataframe est déconseillé si vous avez du vrai big data).\n",
    "\n",
    "L'objet possède aussi un attribut appelé dtypes, appelez cet attribut pour obtenir la liste des colonnes et leur type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+--------------------+------+--------+----+---+----+\n",
      "|dep|      date|lib_dep|             lib_reg|tx_pos|tx_incid|hosp|rea| pos|\n",
      "+---+----------+-------+--------------------+------+--------+----+---+----+\n",
      "|  1|18/03/2020|    Ain|Auvergne et Rhône...|  null|    null|   2|  0|null|\n",
      "|  1|19/03/2020|    Ain|Auvergne et Rhône...|  null|    null|   2|  0|null|\n",
      "|  1|20/03/2020|    Ain|Auvergne et Rhône...|  null|    null|   2|  0|null|\n",
      "|  1|21/03/2020|    Ain|Auvergne et Rhône...|  null|    null|   4|  0|null|\n",
      "|  1|22/03/2020|    Ain|Auvergne et Rhône...|  null|    null|   8|  1|null|\n",
      "|  1|23/03/2020|    Ain|Auvergne et Rhône...|  null|    null|  17|  3|null|\n",
      "|  1|24/03/2020|    Ain|Auvergne et Rhône...|  null|    null|  22|  3|null|\n",
      "|  1|25/03/2020|    Ain|Auvergne et Rhône...|  null|    null|  29|  6|null|\n",
      "|  1|26/03/2020|    Ain|Auvergne et Rhône...|  null|    null|  33|  6|null|\n",
      "|  1|27/03/2020|    Ain|Auvergne et Rhône...|  null|    null|  49|  9|null|\n",
      "|  1|28/03/2020|    Ain|Auvergne et Rhône...|  null|    null|  53| 12|null|\n",
      "|  1|29/03/2020|    Ain|Auvergne et Rhône...|  null|    null|  55| 13|null|\n",
      "|  1|30/03/2020|    Ain|Auvergne et Rhône...|  null|    null|  67| 19|null|\n",
      "|  1|31/03/2020|    Ain|Auvergne et Rhône...|  null|    null|  72| 16|null|\n",
      "|  1|01/04/2020|    Ain|Auvergne et Rhône...|  null|    null|  92| 28|null|\n",
      "|  1|02/04/2020|    Ain|Auvergne et Rhône...|  null|    null| 105| 33|null|\n",
      "|  1|03/04/2020|    Ain|Auvergne et Rhône...|  null|    null| 100| 31|null|\n",
      "|  1|04/04/2020|    Ain|Auvergne et Rhône...|  null|    null| 109| 32|null|\n",
      "|  1|05/04/2020|    Ain|Auvergne et Rhône...|  null|    null| 112| 30|null|\n",
      "|  1|06/04/2020|    Ain|Auvergne et Rhône...|  null|    null| 115| 30|null|\n",
      "+---+----------+-------+--------------------+------+--------+----+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"hdfs://namenode:9000/Bigdata/covid.csv\"\n",
    "test= spark.read.format('csv').options(header= True , sep=\";\").load(path)\n",
    "test.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrameReader' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test\u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://namenode:9000/Bigdata/frequentation-gares.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrameReader' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) lecture avec les entêtes\n",
    "Recharger le même fichier mais cette fois-ci utilisez l'option header pour rajouter les noms de colonnes à votre df.\n",
    "\n",
    "Appelez l'attribut dtypes et comparez la sortie avec celle de la lecture brute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dep;date;lib_dep;lib_reg;tx_pos;tx_incid;hosp;rea;pos', 'string')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = spark.read.format('csv').options(header=True).load(path)\n",
    "test.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (4135978901.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [27], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    df_test = spark.read.options(header='True') \\.csv(\"hdfs://namenode:9000/Bigdata/frequentation-gares.csv\")\u001b[0m\n\u001b[0m                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) lecture avec les types détectés automatiquement\n",
    "Recharger le fichier avec  l'option inferShema.\n",
    "\n",
    "L'option 'inferSchema' permet de transformer les colonnes en types plus précis : entier  / booléens / chaines de caractères... bien sûr spark trouve les types uniquement si le fichier d'origine permet de les trouver de manière simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dep;date;lib_dep;lib_reg;tx_pos;tx_incid;hosp;rea;pos', 'string')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = spark.read.format('csv').options(header=True, inferSchema=True).load(path)\n",
    "test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) lecture avec schéma\n",
    "Il vous permet d'afficher le schéma de votre df, avec pour chaque colonne son nom, son type, et si elle accepte les valeurs nulles ou non. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(dep;date;lib_dep;lib_reg;tx_pos;tx_incid;hosp;rea;pos,StringType,true)))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous avez aussi la méthode printSchema() qui permet d'afficher le shéma du df de manière plus lisible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dep: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- lib_dep: string (nullable = true)\n",
      " |-- lib_reg: string (nullable = true)\n",
      " |-- tx_pos: string (nullable = true)\n",
      " |-- tx_incid: string (nullable = true)\n",
      " |-- hosp: string (nullable = true)\n",
      " |-- rea: string (nullable = true)\n",
      " |-- pos: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) écriture de la dataframe sur le disque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) choix du format : csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Can not create a Path from an empty string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [132], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/readwriter.py:827\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 827\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py:137\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    133\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Can not create a Path from an empty string"
     ]
    }
   ],
   "source": [
    "test.write.format(\"csv\").save(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) choix du format : parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Attribute name \"dep;date;lib_dep;lib_reg;tx_pos;tx_incid;hosp;rea;pos\" contains invalid character(s) among \" ,;{}()\\n\\t=\". Please use alias to rename it.;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/readwriter.py:827\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 827\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py:137\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    133\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Attribute name \"dep;date;lib_dep;lib_reg;tx_pos;tx_incid;hosp;rea;pos\" contains invalid character(s) among \" ,;{}()\\n\\t=\". Please use alias to rename it.;"
     ]
    }
   ],
   "source": [
    "test.write.format(\"parquet\").save(\"./parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) choix du format : json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/10 09:28:40 ERROR TransportClient: Failed to send RPC RPC 5442206741629275941 to /172.19.0.7:43332: java.nio.channels.ClosedChannelException\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/11/10 09:28:40 WARN BlockManagerMasterEndpoint: Error trying to remove broadcast 295 from block manager BlockManagerId(0, 172.19.0.7, 44215, None)\n",
      "java.io.IOException: Failed to send RPC RPC 5442206741629275941 to /172.19.0.7:43332: java.nio.channels.ClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:363)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:340)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:608)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:993)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\t... 12 more\n",
      "22/11/10 09:28:40 ERROR TransportClient: Failed to send RPC RPC 7232903952878449116 to /172.19.0.7:43332: java.nio.channels.ClosedChannelException\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/11/10 09:28:40 WARN BlockManagerMasterEndpoint: Error trying to remove broadcast 292 from block manager BlockManagerId(0, 172.19.0.7, 44215, None)\n",
      "java.io.IOException: Failed to send RPC RPC 7232903952878449116 to /172.19.0.7:43332: java.nio.channels.ClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:363)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:340)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:608)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:993)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\t... 12 more\n",
      "22/11/10 09:28:40 ERROR TransportClient: Failed to send RPC RPC 8047673392509143989 to /172.19.0.7:43332: java.nio.channels.ClosedChannelException\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/11/10 09:28:40 WARN BlockManagerMasterEndpoint: Error trying to remove broadcast 294 from block manager BlockManagerId(0, 172.19.0.7, 44215, None)\n",
      "java.io.IOException: Failed to send RPC RPC 8047673392509143989 to /172.19.0.7:43332: java.nio.channels.ClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:363)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:340)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:608)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:993)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\t... 12 more\n",
      "22/11/10 09:28:40 ERROR TransportClient: Failed to send RPC RPC 5727239859085570813 to /172.19.0.7:43332: java.nio.channels.ClosedChannelException\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/11/10 09:28:40 WARN BlockManagerMasterEndpoint: Error trying to remove broadcast 293 from block manager BlockManagerId(0, 172.19.0.7, 44215, None)\n",
      "java.io.IOException: Failed to send RPC RPC 5727239859085570813 to /172.19.0.7:43332: java.nio.channels.ClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:363)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:340)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:608)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:993)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\t... 12 more\n"
     ]
    }
   ],
   "source": [
    "df.write.save(\"./csv_data\", format=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv  parquet  ville  ville_1.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ./data_velo/Villes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) lecture de différents formats\n",
    "Vous pouvez choisir de lire le df sous un format ou un autre en utilisant l'argument format dans la fonction spark.read.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, casseur: boolean, home: string, id: bigint, salaire: double, sexe: string, sportif: boolean, sportivite: double, statut: string, travail: string, velo_perf_minimale: double, vitesse_a_pied: double, vitesse_a_velo: double]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.load(\"./json_data\", format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96355"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.load(path, format=\"csv\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Calculer des résultats : les actions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) nombre de lignes : count\n",
    "Chargez les fichiers csv contenus dans le dossiers ./data_velo/Cyclistes/ dans un df nommé cyclistes, puis comptez les lignes du dataframe obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '.hdfs://namenode:9000/Bigdata/covid.csv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls .\"hdfs://namenode:9000/Bigdata/covid.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96355"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.load(path, format ('csv')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df   = spark.read.load(path, format=\"csv\")\n",
    "\n",
    "test = spark.read.format('csv').options(header=True,sep = \";\").load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dep', 'string'),\n",
       " ('date', 'string'),\n",
       " ('lib_dep', 'string'),\n",
       " ('lib_reg', 'string'),\n",
       " ('tx_pos', 'string'),\n",
       " ('tx_incid', 'string'),\n",
       " ('hosp', 'string'),\n",
       " ('rea', 'string'),\n",
       " ('pos', 'string')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher le schéma de ce nouveau df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dep: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- lib_dep: string (nullable = true)\n",
      " |-- lib_reg: string (nullable = true)\n",
      " |-- tx_pos: string (nullable = true)\n",
      " |-- tx_incid: string (nullable = true)\n",
      " |-- hosp: string (nullable = true)\n",
      " |-- rea: string (nullable = true)\n",
      " |-- pos: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez 10 lignes du df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+--------------------+------+--------+----+---+----+\n",
      "|dep|      date|lib_dep|             lib_reg|tx_pos|tx_incid|hosp|rea| pos|\n",
      "+---+----------+-------+--------------------+------+--------+----+---+----+\n",
      "|  1|18/03/2020|    Ain|Auvergne et Rhône...|  null|    null|   2|  0|null|\n",
      "|  1|19/03/2020|    Ain|Auvergne et Rhône...|  null|    null|   2|  0|null|\n",
      "|  1|20/03/2020|    Ain|Auvergne et Rhône...|  null|    null|   2|  0|null|\n",
      "|  1|21/03/2020|    Ain|Auvergne et Rhône...|  null|    null|   4|  0|null|\n",
      "|  1|22/03/2020|    Ain|Auvergne et Rhône...|  null|    null|   8|  1|null|\n",
      "|  1|23/03/2020|    Ain|Auvergne et Rhône...|  null|    null|  17|  3|null|\n",
      "|  1|24/03/2020|    Ain|Auvergne et Rhône...|  null|    null|  22|  3|null|\n",
      "|  1|25/03/2020|    Ain|Auvergne et Rhône...|  null|    null|  29|  6|null|\n",
      "|  1|26/03/2020|    Ain|Auvergne et Rhône...|  null|    null|  33|  6|null|\n",
      "|  1|27/03/2020|    Ain|Auvergne et Rhône...|  null|    null|  49|  9|null|\n",
      "+---+----------+-------+--------------------+------+--------+----+---+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) moyenne : agg + colonne + mean\n",
    "A l'aide de la méthode agg(), calculez la moyenne sur la colonne vitesse.\n",
    "\n",
    "Vous pouvez récuperer le résultat avec la méthode collect()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         avg(hosp)|\n",
      "+------------------+\n",
      "|177.33182651991615|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.agg({'hosp':'mean'}).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) quantile approximatifs pour gagner du temps de calcul\n",
    "En statistiques et en théorie des probabilités, les quantiles sont les valeurs qui divisent un jeu de données en intervalles contenant le même nombre de données. Il y a donc un quantile de moins que le nombre de groupes créés. Ainsi les quartiles sont les trois quantiles qui divisent un ensemble de données en quatre groupes de taille égale.\n",
    "\n",
    "La méthode approxQuantile permet de laisser une tolérance a l'erreur ce qui réduit le temps de calul sur d'énormes jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dep;date;lib_dep;lib_reg;tx_pos;tx_incid;hosp;rea;pos: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_quantile(df, erreur_acceptee):\n",
    "    debut            = time.time()\n",
    "    colonne          = \"vitesse\"\n",
    "    quantiles_voulus = [0.25, 0.50, 0.75]\n",
    "    resultat         =  df.approxQuantile(colonne, quantiles_voulus , erreur_acceptee )\n",
    "    fin              = time.time()\n",
    "    delais           = fin -debut\n",
    "    print (\"delais =%.2f sec, quantiles = %s\"%(delais, resultat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delais =5.53 sec, quantiles = [0.030000000000000006, 0.2944550644296354, 0.5931162118010243]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "calcul_quantile(df, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delais =4.78 sec, quantiles = [0.030000000000000006, 0.2944550644296354, 0.6295531740219638]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "calcul_quantile(df, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delais =15.12 sec, quantiles = [0.030000000000000006, 0.3283952876721612, 0.6295531740219638]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "calcul_quantile(df, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload de la dataframe\n",
    "Chargez le fichier villes dans un df nommé villes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- vitesse_a_pied: double (nullable = true)\n",
      " |-- vitesse_a_velo: double (nullable = true)\n",
      " |-- home: string (nullable = true)\n",
      " |-- travail: string (nullable = true)\n",
      " |-- sportif: boolean (nullable = true)\n",
      " |-- casseur: boolean (nullable = true)\n",
      " |-- statut: string (nullable = true)\n",
      " |-- salaire: double (nullable = true)\n",
      " |-- sexe: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sportivite: double (nullable = true)\n",
      " |-- velo_perf_minimale: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "villes =spark.read.load(\"./data_velo/villes/\", format=\"csv\", header=True, inferSchema=\"True\")\n",
    "villes.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4) corrélation\n",
    "En probabilités et en statistique, la corrélation entre plusieurs variables aléatoires ou statistiques est une notion de liaison qui contredit leur indépendance.\n",
    "\n",
    "Calculez la corrélation entre les colonnes age et vitesse_a_velo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06411845578664936"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "villes.corr(col1= 'age', col2= 'vitesse_a_velo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5) covariance\n",
    "Calculez la covariance entre les colonnes age et vitesse_a_velo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5721945755314064"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "villes.cov ('age', 'vitesse_a_velo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6) sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "villes_1_pct = villes.sample(False, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "villes_1_pct.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "villes.exceptAll(villes_1_pct).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7) filter \n",
    "La méthode filter() permet le df selon certaines valeurs dans les colonnes.\n",
    "\n",
    "Utilisez cette méthode pour récuperer seulement les lignes avec le sexe féminin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+-------------------+-------------------+-------+-------+--------------------+------------------+----+---+-------------------+------------------+\n",
      "| id|     vitesse_a_pied|     vitesse_a_velo|               home|            travail|sportif|casseur|              statut|           salaire|sexe|age|         sportivite|velo_perf_minimale|\n",
      "+---+-------------------+-------------------+-------------------+-------------------+-------+-------+--------------------+------------------+----+---+-------------------+------------------+\n",
      "|  3|               0.02|               0.05|(lon:3.79 lat:3.81)|(lon:0.20 lat:0.21)|  false|  false|technicien_de_sur...| 20026.72646423192|   F| 20|                0.1|               0.4|\n",
      "|  4|               0.02|               0.05|(lon:3.39 lat:0.93)|(lon:0.58 lat:0.20)|  false|  false|technicien_de_sur...|15214.584161640825|   F| 35|                0.1|               0.4|\n",
      "|  6|               0.02|               0.05|(lon:0.44 lat:1.45)|(lon:1.85 lat:0.04)|  false|  false|          professeur|30852.120709809133|   F| 79|                0.1|               0.4|\n",
      "|  7| 0.9726853575358816|  2.431713393839704|(lon:0.18 lat:2.60)|(lon:2.16 lat:2.28)|  false|  false|             employe| 54494.87538632937|   F| 46|  4.863426787679408|               0.4|\n",
      "|  9| 0.6295531740219638| 1.5738829350549093|(lon:2.16 lat:2.32)|(lon:3.44 lat:3.54)|  false|  false|technicien_de_sur...|11799.927798039249|   F| 65| 3.1477658701098186|               0.4|\n",
      "| 11|0.25214291732243804|  0.630357293306095|(lon:0.28 lat:2.44)|(lon:1.28 lat:1.40)|  false|  false|             employe|  53716.3729157698|   F| 21|   1.26071458661219|               0.4|\n",
      "| 14| 0.3733552690301726| 0.9333881725754316|(lon:3.87 lat:3.74)|(lon:0.41 lat:0.36)|  false|  false|               cadre|  74308.6319691613|   F| 34|  1.866776345150863|               0.4|\n",
      "| 16|               0.02|               0.05|(lon:1.37 lat:1.09)|(lon:0.45 lat:1.77)|  false|  false|             éboueur|21040.289070523115|   F| 32|                0.1|               0.4|\n",
      "| 17| 1.0608622097751907| 2.6521555244379766|(lon:3.68 lat:0.79)|(lon:2.67 lat:3.66)|  false|  false|technicien_de_sur...|16946.365288912493|   F| 16|  5.304311048875953|               0.4|\n",
      "| 18|               0.02|               0.05|(lon:0.51 lat:2.32)|(lon:1.01 lat:0.40)|  false|  false|technicien_de_sur...|22099.145463261266|   F| 24|                0.1|               0.4|\n",
      "| 22| 0.5931162118010243| 1.4827905295025607|(lon:1.41 lat:2.43)|(lon:1.49 lat:1.44)|  false|  false|             employe|38906.393030229774|   F| 36|  2.965581059005121|               0.4|\n",
      "| 23| 0.1431743884246346| 0.3579359710615865|(lon:0.80 lat:2.82)|(lon:0.34 lat:3.77)|  false|  false|             employe|29085.686473822872|   F| 78| 0.7158719421231731|               0.4|\n",
      "| 27| 0.4804764997211946| 1.2011912493029866|(lon:0.78 lat:2.62)|(lon:2.30 lat:0.01)|  false|  false|technicien_de_sur...|18234.071705216207|   F| 59|  2.402382498605973|               0.4|\n",
      "| 29|0.07468040314496042|0.18670100786240107|(lon:3.75 lat:3.12)|(lon:2.30 lat:3.92)|  false|  false|          reserviste|29515.863019061188|   F| 67| 0.3734020157248021|               0.4|\n",
      "| 30| 0.5525664880322532| 1.3814162200806333|(lon:1.08 lat:3.77)|(lon:0.72 lat:3.32)|  false|  false|             employe|27046.171282769843|   F| 81| 2.7628324401612665|               0.4|\n",
      "| 34|0.03887047976999849|0.09717619942499622|(lon:2.83 lat:0.56)|(lon:3.25 lat:1.60)|  false|  false|technicien_de_sur...|17175.758356245642|   F| 21|0.19435239884999245|               0.4|\n",
      "| 42| 0.6992238313299156| 1.7480595783247892|(lon:3.80 lat:3.99)|(lon:2.13 lat:1.65)|  false|  false|             employe| 19383.17526928032|   F| 21| 3.4961191566495784|               0.4|\n",
      "| 47|0.18444538804220728| 0.4611134701055182|(lon:1.74 lat:3.62)|(lon:2.96 lat:2.23)|  false|  false|          reserviste| 30798.10624851897|   F| 58| 0.9222269402110364|               0.4|\n",
      "| 48| 0.5652688776055729|  1.413172194013932|(lon:3.51 lat:2.24)|(lon:0.27 lat:1.34)|  false|  false|             éboueur|12133.681930505132|   F| 56| 2.8263443880278643|               0.4|\n",
      "| 49| 0.9920385410084079|   2.48009635252102|(lon:0.13 lat:2.04)|(lon:1.35 lat:0.63)|  false|  false|technicien_de_sur...| 9511.945356442959|   F| 58|   4.96019270504204|               0.4|\n",
      "+---+-------------------+-------------------+-------------------+-------------------+-------+-------+--------------------+------------------+----+---+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "villes.filter(\"sexe ='F'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- vitesse_a_pied: double (nullable = true)\n",
      " |-- vitesse_a_velo: double (nullable = true)\n",
      " |-- home: string (nullable = true)\n",
      " |-- travail: string (nullable = true)\n",
      " |-- sportif: boolean (nullable = true)\n",
      " |-- casseur: boolean (nullable = true)\n",
      " |-- statut: string (nullable = true)\n",
      " |-- salaire: double (nullable = true)\n",
      " |-- sexe: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sportivite: double (nullable = true)\n",
      " |-- velo_perf_minimale: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "villes =spark.read.load(\"./data_velo/villes/\", format=\"csv\", header=True, inferSchema=\"True\")\n",
    "villes.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Transformer la données : les transformations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations : demandent à être suivi par un collect ou une action (count par exemple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) obtenir des statistiques sur les colonnes numériques\n",
    "La méthode describe() permet de calculer les statistiques récapitulatives d'une ou plusieurs colonnes numériques dans un df. Si le nom des colonnes n'est pas spécifié, la méthode calculera des statistiques récapitulatives pour toutes les colonnes numériques présentes dans le df.\n",
    "\n",
    "Afficher les statistiques de la colonne age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(summary='count', summary='5', sexe='3', age='5'),\n",
       " Row(summary='mean', summary=None, sexe='50.0', age='43.43979797464467'),\n",
       " Row(summary='stddev', summary=None, sexe='NaN', age='27.13153036418277'),\n",
       " Row(summary='min', summary='count', sexe='50', age='16'),\n",
       " Row(summary='max', summary='stddev', sexe='H', age='83')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "villes.describe([\"sexe\", \"age\"]).describe().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) groupby\n",
    "La méthode groupBy() suivie de la methode agg() permet de grouper le df selon les catgories d'une ou plusieurs colonnes pour faire des calculs sur ces catégories.\n",
    "\n",
    "Calculez la moyenne de la colonnes sportivité selon le sexe des personnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|sexe|   avg(sportivite)|\n",
      "+----+------------------+\n",
      "|   F|1.8410619134680517|\n",
      "|   H|1.6356186755623958|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "villes.groupBy('sexe').agg({\"sportivite\":'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculez la moyenne de la colonne age et la valeur max de la colonne sportivité par sexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+------------------+\n",
      "|sexe|  max(sportivite)|          avg(age)|\n",
      "+----+-----------------+------------------+\n",
      "|   F|5.304311048875953|46.095238095238095|\n",
      "|   H|7.814024407120282| 50.06896551724138|\n",
      "+----+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "villes.groupBy('sexe').agg({\"age\":'mean' , \"sportivite\" :'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculez la moyenne des colonnes vitesse_a_pied et vitesse_a_velo par sexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:===================================================>    (69 + 2) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-------------------+\n",
      "|sexe|avg(vitesse_a_velo)|avg(vitesse_a_pied)|\n",
      "+----+-------------------+-------------------+\n",
      "|   F| 0.9205309567340259|0.36821238269361034|\n",
      "|   H| 1.3084949404499162| 0.4906856026687184|\n",
      "+----+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "villes.groupBy('sexe').agg({\"vitesse_a_pied\":'mean' , \"vitesse_a_velo\" :'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) summary \n",
    "La méthode summary() permet des faire des calculs statistiques de base sur toutes les colonnes du df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary='count', id='50', vitesse_a_pied='50', vitesse_a_velo='50', home='50', travail='50', statut='50', salaire='50', sexe='50', age='50', sportivite='50', velo_perf_minimale='50'),\n",
       " Row(summary='max', id='51', vitesse_a_pied='2.344207322136085', vitesse_a_velo='6.251219525696226', home='(lon:3.90 lat:0.62)', travail='(lon:3.91 lat:3.22)', statut='éboueur', salaire='148702.7189509448', sexe='H', age='83', sportivite='7.814024407120282', velo_perf_minimale='0.4')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "villes.summary(\"count\", \"max\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4) union de dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ajouter les colonnes les unes à côté des autres : join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- vitesse_a_pied: double (nullable = true)\n",
      " |-- vitesse_a_velo: double (nullable = true)\n",
      " |-- home: string (nullable = true)\n",
      " |-- travail: string (nullable = true)\n",
      " |-- sportif: boolean (nullable = true)\n",
      " |-- casseur: boolean (nullable = true)\n",
      " |-- statut: string (nullable = true)\n",
      " |-- salaire: double (nullable = true)\n",
      " |-- sexe: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sportivite: double (nullable = true)\n",
      " |-- velo_perf_minimale: double (nullable = true)\n",
      " |-- vitesse_a_pied: double (nullable = true)\n",
      " |-- vitesse_a_velo: double (nullable = true)\n",
      " |-- home: string (nullable = true)\n",
      " |-- travail: string (nullable = true)\n",
      " |-- sportif: boolean (nullable = true)\n",
      " |-- casseur: boolean (nullable = true)\n",
      " |-- statut: string (nullable = true)\n",
      " |-- salaire: double (nullable = true)\n",
      " |-- sexe: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sportivite: double (nullable = true)\n",
      " |-- velo_perf_minimale: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "villes.join(villes, on=\"id\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ajouter les lignes les unes sous les autres : union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "villes.unionByName(villes).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5) filtre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "villes.where(villes.sexe==\"F\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6) concaténation de colonne : F.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql       import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ici reprendre le df cyclistes.\n",
    "\n",
    "Utiliser les méthodes withColumn() et F.concat() pour ajouter une colonne au df qui contiendra la concatenation des valeurs des colonnes id et sur_velo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(id=12, timestamp='2018-01-01 00:01:00', sur_velo=False, velo='False', vitesse=0.030000000000000006, position='(lon:2.07 lat:1.24)', destination_finale='False', id_sur_velo='12false')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./data_velo/Cyclistes/*.csv\" \n",
    "tous_les_cyclistes = spark.read.format(\"csv\").option(\"header\", \"true\").load(path, inferSchema=True)\n",
    "tous_les_cyclistes.count()\n",
    "tous_les_cyclistes.withColumn(\"id_sur_velo\", F.concat(tous_les_cyclistes.id,  tous_les_cyclistes.sur_velo)).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) udf \n",
    "Il est possible d'enregistrer des fonctions python que l'on écrit nous même pour les appliquer sur une colonne d'une dataframe, c'est ce qu'on appelle les udf, pour User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types     import *\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(returnType = FloatType())\n",
    "def cube(colonne):\n",
    "    return colonne*colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|cube(salaire)|\n",
      "+-------------+\n",
      "|1.82702162E10|\n",
      "|  4.0106976E8|\n",
      "| 2.31483568E8|\n",
      "| 3.32549088E8|\n",
      "|  9.5185338E8|\n",
      "| 2.96969139E9|\n",
      "| 1.12880461E9|\n",
      "| 1.39238304E8|\n",
      "|  1.0977216E9|\n",
      "|  2.8854487E9|\n",
      "|  6.4748621E8|\n",
      "| 1.22958438E9|\n",
      "|  5.5217725E9|\n",
      "| 2.57463706E9|\n",
      "|  4.4269376E8|\n",
      "| 2.87179296E8|\n",
      "| 4.88372224E8|\n",
      "|  6.9554765E9|\n",
      "|  6.1511916E9|\n",
      "|   5.757398E9|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "villes.select(cube(\"salaire\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|cube(salaire)|\n",
      "+-------------+\n",
      "|1.82702162E10|\n",
      "|  4.0106976E8|\n",
      "| 2.31483568E8|\n",
      "| 3.32549088E8|\n",
      "|  9.5185338E8|\n",
      "| 2.96969139E9|\n",
      "| 1.12880461E9|\n",
      "| 1.39238304E8|\n",
      "|  1.0977216E9|\n",
      "|  2.8854487E9|\n",
      "|  6.4748621E8|\n",
      "| 1.22958438E9|\n",
      "|  5.5217725E9|\n",
      "| 2.57463706E9|\n",
      "|  4.4269376E8|\n",
      "| 2.87179296E8|\n",
      "| 4.88372224E8|\n",
      "|  6.9554765E9|\n",
      "|  6.1511916E9|\n",
      "|   5.757398E9|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "villes.select(cube(villes.salaire)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(cube(salaire)=18270216192.0),\n",
       " Row(cube(salaire)=401069760.0),\n",
       " Row(cube(salaire)=231483568.0),\n",
       " Row(cube(salaire)=332549088.0),\n",
       " Row(cube(salaire)=951853376.0),\n",
       " Row(cube(salaire)=2969691392.0),\n",
       " Row(cube(salaire)=1128804608.0),\n",
       " Row(cube(salaire)=139238304.0),\n",
       " Row(cube(salaire)=1097721600.0),\n",
       " Row(cube(salaire)=2885448704.0),\n",
       " Row(cube(salaire)=647486208.0),\n",
       " Row(cube(salaire)=1229584384.0),\n",
       " Row(cube(salaire)=5521772544.0),\n",
       " Row(cube(salaire)=2574637056.0),\n",
       " Row(cube(salaire)=442693760.0),\n",
       " Row(cube(salaire)=287179296.0),\n",
       " Row(cube(salaire)=488372224.0),\n",
       " Row(cube(salaire)=6955476480.0),\n",
       " Row(cube(salaire)=6151191552.0),\n",
       " Row(cube(salaire)=5757398016.0),\n",
       " Row(cube(salaire)=1513707392.0),\n",
       " Row(cube(salaire)=845977152.0),\n",
       " Row(cube(salaire)=15607006208.0),\n",
       " Row(cube(salaire)=279298720.0),\n",
       " Row(cube(salaire)=658956352.0),\n",
       " Row(cube(salaire)=332481376.0),\n",
       " Row(cube(salaire)=11568824320.0),\n",
       " Row(cube(salaire)=871186176.0),\n",
       " Row(cube(salaire)=731495360.0),\n",
       " Row(cube(salaire)=22112497664.0),\n",
       " Row(cube(salaire)=1202230528.0),\n",
       " Row(cube(salaire)=423060096.0),\n",
       " Row(cube(salaire)=295006688.0),\n",
       " Row(cube(salaire)=1184019968.0),\n",
       " Row(cube(salaire)=229316384.0),\n",
       " Row(cube(salaire)=4948908544.0),\n",
       " Row(cube(salaire)=880376512.0),\n",
       " Row(cube(salaire)=993021504.0),\n",
       " Row(cube(salaire)=332946784.0),\n",
       " Row(cube(salaire)=155353856.0),\n",
       " Row(cube(salaire)=375707488.0),\n",
       " Row(cube(salaire)=421229280.0),\n",
       " Row(cube(salaire)=454536128.0),\n",
       " Row(cube(salaire)=5381703680.0),\n",
       " Row(cube(salaire)=596220544.0),\n",
       " Row(cube(salaire)=948523328.0),\n",
       " Row(cube(salaire)=147226240.0),\n",
       " Row(cube(salaire)=90477104.0),\n",
       " Row(cube(salaire)=988872128.0),\n",
       " Row(cube(salaire)=580938816.0)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "villes.select(cube(\"salaire\")).collect()\n",
    "#villes.withColumn(\"salaire\",cube(\"salaire\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6)\tEtude de cas : analyse des fichiers de logs des cyclistes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql       import functions as F\n",
    "from pyspark.sql.types     import *\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1)  Charger la donnée dans un df nommé cycliste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "cyclistes =spark.read.load(\"./data_velo/Cyclistes/\",format = \"csv\", header=True, inferSchema=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2) vérifier le nombre de cycles\n",
    "\n",
    "Comptez le nombre d'id uniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyclistes.select(\"id\").distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3) transformer les timestamp en date\n",
    "\n",
    "Voici une fonction qui permert de récuperer la date sous forme de chaîne de caractère dans la colonne timestamps pour la transformer en date exploitable en tant que telle.\n",
    "\n",
    "Créez une nouvelle colonne dans votre df cycliste stockant le résultat de cette fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "@udf(returnType = TimestampType())\n",
    "def transform_timestamp_in_date(timestamp):\n",
    "    from datetime import datetime\n",
    "    return datetime.strptime(str(timestamp), \"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------+-----+--------------------+-------------------+------------------+-------------------+\n",
      "| id|          timestamp|sur_velo| velo|             vitesse|           position|destination_finale|         Timestamp2|\n",
      "+---+-------------------+--------+-----+--------------------+-------------------+------------------+-------------------+\n",
      "| 12|2018-01-01 00:01:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:01:00|\n",
      "| 12|2018-01-01 00:02:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:02:00|\n",
      "| 12|2018-01-01 00:03:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:03:00|\n",
      "| 12|2018-01-01 00:04:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:04:00|\n",
      "| 12|2018-01-01 00:05:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:05:00|\n",
      "| 12|2018-01-01 00:06:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:06:00|\n",
      "| 12|2018-01-01 00:07:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:07:00|\n",
      "| 12|2018-01-01 00:08:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:08:00|\n",
      "| 12|2018-01-01 00:09:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:09:00|\n",
      "| 12|2018-01-01 00:10:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:10:00|\n",
      "| 12|2018-01-01 00:11:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:11:00|\n",
      "| 12|2018-01-01 00:12:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:12:00|\n",
      "| 12|2018-01-01 00:13:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:13:00|\n",
      "| 12|2018-01-01 00:14:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:14:00|\n",
      "| 12|2018-01-01 00:15:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:15:00|\n",
      "| 12|2018-01-01 00:16:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:16:00|\n",
      "| 12|2018-01-01 00:17:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:17:00|\n",
      "| 12|2018-01-01 00:18:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:18:00|\n",
      "| 12|2018-01-01 00:19:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:19:00|\n",
      "| 12|2018-01-01 00:20:00|   false|False|0.030000000000000006|(lon:2.07 lat:1.24)|             False|2018-01-01 00:20:00|\n",
      "+---+-------------------+--------+-----+--------------------+-------------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cyclistes.withColumn(\"Timestamp2\",transform_timestamp_in_date(cyclistes['timestamp'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vérifier les comptes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4) Durée des trajets par id.\n",
    "\n",
    "A partir d'ici, il s'agit de traiter votre donnée pour récupérer la durée de chaque trajet effectué par chaque id.\n",
    "\n",
    "1) trouvez les dates min/max par état de sur_velo, puis par id ET par état de sur_velo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 162:===================================================> (194 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------------+-------------------+\n",
      "| id|sur_velo|     min(timestamp)|     max(timestamp)|\n",
      "+---+--------+-------------------+-------------------+\n",
      "|  2|    true|2018-01-01 12:33:00|2018-01-31 20:02:00|\n",
      "|  2|   false|2018-01-01 00:01:00|2018-02-01 00:00:00|\n",
      "|  3|   false|2018-01-01 00:01:00|2018-02-01 00:00:00|\n",
      "|  3|    true|2018-01-06 06:24:00|2018-01-28 19:33:00|\n",
      "|  4|    true|2018-01-06 09:48:00|2018-01-28 15:19:00|\n",
      "|  4|   false|2018-01-01 00:01:00|2018-02-01 00:00:00|\n",
      "|  5|   false|2018-01-01 00:01:00|2018-02-01 00:00:00|\n",
      "|  5|    true|2018-01-06 12:35:00|2018-01-28 16:04:00|\n",
      "|  6|    true|2018-01-01 07:07:00|2018-01-30 20:01:00|\n",
      "|  6|   false|2018-01-01 00:01:00|2018-02-01 00:00:00|\n",
      "|  7|   false|2018-01-01 00:01:00|2018-02-01 00:00:00|\n",
      "|  7|    true|2018-01-01 06:33:00|2018-01-31 18:43:00|\n",
      "|  8|    true|2018-01-06 05:14:00|2018-01-28 19:44:00|\n",
      "|  8|   false|2018-01-01 00:01:00|2018-02-01 00:00:00|\n",
      "|  9|    true|2018-01-06 06:54:00|2018-01-28 14:03:00|\n",
      "|  9|   false|2018-01-01 00:01:00|2018-02-01 00:00:00|\n",
      "| 10|    true|2018-01-01 08:35:00|2018-01-30 17:32:00|\n",
      "| 10|   false|2018-01-01 00:01:00|2018-02-01 00:00:00|\n",
      "| 11|    true|2018-01-01 07:48:00|2018-01-31 14:05:00|\n",
      "| 11|   false|2018-01-01 00:01:00|2018-02-01 00:00:00|\n",
      "+---+--------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cyclistes.groupby([\"id\",\"sur_velo\"]).agg(F.min(cyclistes[\"timestamp\"]) , F.max(cyclistes[\"timestamp\"])).sort(\"id\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+\n",
      "|sur_velo|     min(timestamp)|     max(timestamp)|\n",
      "+--------+-------------------+-------------------+\n",
      "|    true|2018-01-01 01:47:00|2018-01-31 21:32:00|\n",
      "|   false|2018-01-01 00:01:00|2018-02-01 00:00:00|\n",
      "+--------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cyclistes.groupby([\"sur_velo\"]).agg(F.min(cyclistes[\"timestamp\"]) , F.max(cyclistes[\"timestamp\"])).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5) détecter les changements d'états \"sur un vélo\" ou pas : window et lag\n",
    "2) Le résultat n'est pas trés pertinent, il faudrait plutôt le début et la fin de chaque trajet par id. Pour cela, il faudrait détecter les changements d'états \"sur_vélo\".\n",
    "Utilisez la classe Window() et la fonction F.lag() pour créer une nouvelle colonne que vous appellerez changement, contenant un 0 si l'état précedent de sur_velo est le même et un 1 si l'état vient de changer (fonction changement() ci-dessous) pour chaque id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType = IntegerType())\n",
    "def changement(etat_actuel, etat_precedent):\n",
    "    \"\"\"\n",
    "    Détecte si les deux états sont différent.\n",
    "    \n",
    "    Parametres :\n",
    "        etat_actuel : valeur sur la ligne courante\n",
    "                      renvoyée par F.lag (0)\n",
    "        etat_precedent : valeur sur la ligne précédente\n",
    "                      renvoyée par F.lag(1)\n",
    "    Return: 0 s'ils sont égaux, 1 s'il y a une différence\n",
    "    \"\"\"\n",
    "    if etat_precedent == None:\n",
    "        return 0\n",
    "    if etat_precedent == etat_actuel:\n",
    "        return 0\n",
    "    if etat_actuel != etat_precedent:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "w = Window.orderBy([\"id\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collecter les données d'un id et verifier sa position lors des changements d'etats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (462676745.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [14], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    cyclistes.select * from w where \"id\"\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6) somme partielle par sous groupe : windows\n",
    "Grâce à la fonction window appliquez la fonction somme() sur la colonne changement pour numeroter les trajets pour chaque id et stocker les résulats dans une nouvelle colonne appelée numero_de_trajet.\n",
    "\n",
    "\n",
    "[windows fonctions](https://www.quantmetry.com/blog/window-functions-spark/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType = IntegerType())\n",
    "def somme(indice_actuel, indice_precedent):\n",
    "    if indice_precedent == None:\n",
    "        return 0\n",
    "    return indice_actuel + indice_precedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7) Calculer la durée du trajet\n",
    "\n",
    "Il suffit maintenant de repêter la première étape, c'est a dire récupérer la début et la fin de chaque trajet pour chaque id. Puis calculer la durée des trajets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) datavisualisation\n",
    "Convertissez votre dataframe pyspark en dataframe pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide de la librairie seaborn, réalisez un graphique en barre montrant la durée de tout les trajets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire le même graphique mais cette fois-ci, faire en sorte qu'on puisse choisir un id et afficher seulement les trajets de cet id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegardez votre dataset trajets au format csv dans le dossier data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
